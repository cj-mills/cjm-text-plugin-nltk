{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c373bb64",
   "metadata": {},
   "source": [
    "# NLTK Plugin\n",
    "\n",
    "> Plugin implementation for NLTK-based text processing with character-level span tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d97ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "from cjm_text_plugin_system.plugin_interface import TextProcessingPlugin\n",
    "from cjm_text_plugin_system.core import TextProcessResult, TextSpan\n",
    "from cjm_text_plugin_system.storage import TextProcessStorage\n",
    "from cjm_plugin_system.utils.hashing import hash_bytes\n",
    "from cjm_plugin_system.utils.validation import (\n",
    "    dict_to_config, config_to_dict, dataclass_to_jsonschema,\n",
    "    SCHEMA_TITLE, SCHEMA_DESC, SCHEMA_ENUM\n",
    ")\n",
    "from cjm_text_plugin_nltk.meta import get_plugin_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687955b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class NLTKPluginConfig:\n",
    "    \"\"\"Configuration for NLTK text processing plugin.\"\"\"\n",
    "    tokenizer: str = field(\n",
    "        default=\"punkt\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Tokenizer\",\n",
    "            SCHEMA_DESC: \"NLTK tokenizer to use for sentence splitting\",\n",
    "            SCHEMA_ENUM: [\"punkt\"]\n",
    "        }\n",
    "    )\n",
    "    language: str = field(\n",
    "        default=\"english\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Language\",\n",
    "            SCHEMA_DESC: \"Language for tokenization (affects sentence boundary detection)\",\n",
    "            SCHEMA_ENUM: [\"english\", \"german\", \"french\", \"spanish\", \"italian\", \"portuguese\", \"dutch\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330602bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NLTKPlugin(TextProcessingPlugin):\n",
    "    \"\"\"NLTK-based text processing plugin with character-level span tracking.\"\"\"\n",
    "    \n",
    "    config_class = NLTKPluginConfig\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the NLTK plugin.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config: NLTKPluginConfig = None\n",
    "        self._tokenizer: PunktSentenceTokenizer = None\n",
    "        self._nltk_data_dir: Optional[str] = None\n",
    "        self.storage: Optional[TextProcessStorage] = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:  # Plugin name identifier\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"nltk_text\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str:  # Plugin version string\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "\n",
    "    def get_current_config(self) -> Dict[str, Any]:  # Current configuration as dictionary\n",
    "        \"\"\"Return current configuration state.\"\"\"\n",
    "        if not self.config:\n",
    "            return {}\n",
    "        return config_to_dict(self.config)\n",
    "\n",
    "    def get_config_schema(self) -> Dict[str, Any]:  # JSON Schema for configuration\n",
    "        \"\"\"Return JSON Schema for UI generation.\"\"\"\n",
    "        return dataclass_to_jsonschema(NLTKPluginConfig)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_dataclass() -> NLTKPluginConfig:  # Configuration dataclass\n",
    "        \"\"\"Return dataclass describing the plugin's configuration options.\"\"\"\n",
    "        return NLTKPluginConfig\n",
    "    \n",
    "    def _ensure_nltk_data(self) -> None:\n",
    "        \"\"\"Ensure required NLTK data packages are downloaded to the configured directory.\"\"\"\n",
    "        # Get NLTK data directory from environment (set by manifest env_vars)\n",
    "        nltk_data_dir = os.environ.get(\"NLTK_DATA\")\n",
    "        \n",
    "        if nltk_data_dir:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "            \n",
    "            # Replace NLTK's search path to ONLY use our directory\n",
    "            # This prevents NLTK from finding/using data in ~/nltk_data\n",
    "            nltk.data.path = [nltk_data_dir]\n",
    "            \n",
    "            self._nltk_data_dir = nltk_data_dir\n",
    "            self.logger.info(f\"Using NLTK data directory: {nltk_data_dir}\")\n",
    "            \n",
    "            # Check if data exists in OUR directory specifically\n",
    "            punkt_path = os.path.join(nltk_data_dir, \"tokenizers\", \"punkt\")\n",
    "            punkt_tab_path = os.path.join(nltk_data_dir, \"tokenizers\", \"punkt_tab\")\n",
    "            \n",
    "            if not os.path.exists(punkt_path):\n",
    "                self.logger.info(f\"Downloading NLTK 'punkt' tokenizer to {nltk_data_dir}...\")\n",
    "                nltk.download('punkt', quiet=True, download_dir=nltk_data_dir)\n",
    "            \n",
    "            if not os.path.exists(punkt_tab_path):\n",
    "                self.logger.info(f\"Downloading NLTK 'punkt_tab' tokenizer to {nltk_data_dir}...\")\n",
    "                nltk.download('punkt_tab', quiet=True, download_dir=nltk_data_dir)\n",
    "        else:\n",
    "            # No custom directory - use NLTK defaults\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                self.logger.info(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "                nltk.download('punkt', quiet=True)\n",
    "            \n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt_tab')\n",
    "            except LookupError:\n",
    "                self.logger.info(\"Downloading NLTK 'punkt_tab' tokenizer...\")\n",
    "                nltk.download('punkt_tab', quiet=True)\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Any] = None  # Configuration dataclass, dict, or None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize or re-configure the plugin (idempotent).\"\"\"\n",
    "        # Parse new config\n",
    "        new_config = dict_to_config(NLTKPluginConfig, config or {})\n",
    "        \n",
    "        # Check for changes if already running\n",
    "        if self.config:\n",
    "            if self.config.language != new_config.language:\n",
    "                self.logger.info(f\"Config change: Language {self.config.language} -> {new_config.language}\")\n",
    "                self._tokenizer = None  # Reset tokenizer for new language\n",
    "        \n",
    "        # Apply new config\n",
    "        self.config = new_config\n",
    "        \n",
    "        # Ensure NLTK data is available\n",
    "        self._ensure_nltk_data()\n",
    "        \n",
    "        # Initialize standardized storage\n",
    "        db_path = get_plugin_metadata()[\"db_path\"]\n",
    "        self.storage = TextProcessStorage(db_path)\n",
    "        \n",
    "        self.logger.info(f\"Initialized NLTK plugin with language '{self.config.language}'\")\n",
    "    \n",
    "    def _get_tokenizer(self) -> PunktSentenceTokenizer:\n",
    "        \"\"\"Get or create the sentence tokenizer (lazy loading).\"\"\"\n",
    "        if self._tokenizer is None:\n",
    "            self._tokenizer = PunktSentenceTokenizer()\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        action: str = \"split_sentences\",  # Operation: 'split_sentences'\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Any]:  # JSON-serializable result\n",
    "        \"\"\"Execute a text processing operation.\"\"\"\n",
    "        if action == \"split_sentences\":\n",
    "            text = kwargs.pop(\"text\", \"\")\n",
    "            job_id = kwargs.pop(\"job_id\", str(uuid4()))\n",
    "            result = self.split_sentences(text, **kwargs)\n",
    "            \n",
    "            # Serialize for IPC\n",
    "            spans_data = [s.to_dict() for s in result.spans]\n",
    "            \n",
    "            # Save to standardized storage\n",
    "            input_hash = hash_bytes(text.encode())\n",
    "            try:\n",
    "                self.storage.save(\n",
    "                    job_id=job_id,\n",
    "                    input_text=text,\n",
    "                    input_hash=input_hash,\n",
    "                    spans=spans_data,\n",
    "                    metadata=result.metadata\n",
    "                )\n",
    "                self.logger.info(f\"Saved result to DB (Job: {job_id})\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to save to DB: {e}\")\n",
    "            \n",
    "            return {\n",
    "                \"spans\": spans_data,\n",
    "                \"metadata\": result.metadata\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action: {action}\")\n",
    "\n",
    "    def split_sentences(\n",
    "        self,\n",
    "        text: str,  # Input text to split into sentences\n",
    "        **kwargs\n",
    "    ) -> TextProcessResult:  # Result with TextSpan objects containing character indices\n",
    "        \"\"\"Split text into sentence spans with accurate character positions.\"\"\"\n",
    "        tokenizer = self._get_tokenizer()\n",
    "        \n",
    "        # Get (start, end) tuples using span_tokenize\n",
    "        span_indices = list(tokenizer.span_tokenize(text))\n",
    "        \n",
    "        text_spans: List[TextSpan] = []\n",
    "        for start, end in span_indices:\n",
    "            span_text = text[start:end]\n",
    "            text_spans.append(TextSpan(\n",
    "                text=span_text,\n",
    "                start_char=start,\n",
    "                end_char=end,\n",
    "                label=\"sentence\"\n",
    "            ))\n",
    "        \n",
    "        return TextProcessResult(\n",
    "            spans=text_spans,\n",
    "            metadata={\n",
    "                \"processor\": self.name,\n",
    "                \"tokenizer\": self.config.tokenizer if self.config else \"punkt\",\n",
    "                \"language\": self.config.language if self.config else \"english\",\n",
    "                \"nltk_data_dir\": self._nltk_data_dir\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        self._tokenizer = None\n",
    "        self.logger.info(\"NLTK plugin cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24763a12",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbdf99-3755-4922-ad49-819514fc1499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin name: nltk_text\n",
      "Plugin version: 1.0.0\n",
      "Config class: NLTKPluginConfig\n",
      "Available languages:\n",
      "  - english\n",
      "  - german\n",
      "  - french\n",
      "  - spanish\n",
      "  - italian\n",
      "  - portuguese\n",
      "  - dutch\n",
      "Current config: {'tokenizer': 'punkt', 'language': 'english'}\n",
      "JSON Schema for NLTKPluginConfig:\n",
      "{\n",
      "  \"name\": \"NLTKPluginConfig\",\n",
      "  \"title\": \"NLTKPluginConfig\",\n",
      "  \"description\": \"Configuration for NLTK text processing plugin.\",\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"tokenizer\": {\n",
      "      \"type\": \"string\",\n",
      "      \"title\": \"Tokenizer\",\n",
      "      \"description\": \"NLTK tokenizer to use for sentence splitting\",\n",
      "      \"enum\": [\n",
      "        \"punkt\"\n",
      "      ],\n",
      "      \"default\": \"punkt\"\n",
      "    },\n",
      "    \"language\": {\n",
      "      \"type\": \"string\",\n",
      "      \"title\": \"Language\",\n",
      "      \"description\": \"Language for tokenization (affects sentence boundary detection)\",\n",
      "      \"enum\": [\n",
      "        \"english\",\n",
      "        \"german\",\n",
      "        \"french\",\n",
      "        \"spanish\",\n",
      "        \"italian\",\n",
      "        \"portuguese\",\n",
      "        \"dutch\"\n",
      "      ],\n",
      "      \"default\": \"english\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = NLTKPlugin()\n",
    "\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Config class: {plugin.config_class.__name__}\")\n",
    "\n",
    "# Test configuration dataclass\n",
    "from dataclasses import fields\n",
    "\n",
    "print(\"Available languages:\")\n",
    "lang_field = next(f for f in fields(NLTKPluginConfig) if f.name == \"language\")\n",
    "for lang in lang_field.metadata.get(SCHEMA_ENUM, []):\n",
    "    print(f\"  - {lang}\")\n",
    "\n",
    "# Test initialization\n",
    "plugin.initialize({\"language\": \"english\"})\n",
    "\n",
    "current_config = plugin.get_current_config()\n",
    "print(f\"Current config: {current_config}\")\n",
    "\n",
    "# Test get_config_schema for UI generation\n",
    "import json\n",
    "\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"JSON Schema for NLTKPluginConfig:\")\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191eb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Hello world. How are you? I am fine! This is a test.'\n",
      "Spans found: 4\n",
      "Metadata: {'processor': 'nltk_text', 'tokenizer': 'punkt', 'language': 'english', 'nltk_data_dir': None}\n",
      "  0: 'Hello world.' [0:12]\n",
      "  1: 'How are you?' [13:25]\n",
      "  2: 'I am fine!' [26:36]\n",
      "  3: 'This is a test.' [37:52]\n"
     ]
    }
   ],
   "source": [
    "# Test split_sentences directly\n",
    "text = \"Hello world. How are you? I am fine! This is a test.\"\n",
    "result = plugin.split_sentences(text)\n",
    "\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Spans found: {len(result.spans)}\")\n",
    "print(f\"Metadata: {result.metadata}\")\n",
    "\n",
    "for i, span in enumerate(result.spans):\n",
    "    print(f\"  {i}: '{span.text}' [{span.start_char}:{span.end_char}]\")\n",
    "    # Verify mapping back to original\n",
    "    assert text[span.start_char:span.end_char] == span.text, f\"Mismatch at span {i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cefb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON result from execute():\n",
      "  spans: 4 items\n",
      "  metadata: {'processor': 'nltk_text', 'tokenizer': 'punkt', 'language': 'english', 'nltk_data_dir': None}\n",
      "    - 'Hello world.' [0:12]\n",
      "    - 'How are you?' [13:25]\n",
      "    - 'I am fine!' [26:36]\n",
      "    - 'This is a test.' [37:52]\n"
     ]
    }
   ],
   "source": [
    "# Test execute() dispatcher (as Worker would call it)\n",
    "json_result = plugin.execute(action=\"split_sentences\", text=text)\n",
    "\n",
    "print(f\"JSON result from execute():\")\n",
    "print(f\"  spans: {len(json_result['spans'])} items\")\n",
    "print(f\"  metadata: {json_result['metadata']}\")\n",
    "\n",
    "for span_dict in json_result['spans']:\n",
    "    print(f\"    - {span_dict['text']!r} [{span_dict['start_char']}:{span_dict['end_char']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b982a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-paragraph text - 6 sentences found:\n",
      "  0: [  0: 16] 'First paragraph.'\n",
      "  1: [ 17: 38] 'It has two sentences.'\n",
      "  2: [ 40: 69] 'Second paragraph starts here.'\n",
      "  3: [ 70: 89] 'And continues here!'\n",
      "  4: [ 91:129] 'Third paragraph: What about questions?'\n",
      "  5: [130:144] 'They work too.'\n"
     ]
    }
   ],
   "source": [
    "# Test with multi-paragraph text\n",
    "multi_text = \"\"\"First paragraph. It has two sentences.\n",
    "\n",
    "Second paragraph starts here. And continues here!\n",
    "\n",
    "Third paragraph: What about questions? They work too.\"\"\"\n",
    "\n",
    "result = plugin.split_sentences(multi_text)\n",
    "print(f\"Multi-paragraph text - {len(result.spans)} sentences found:\")\n",
    "for i, span in enumerate(result.spans):\n",
    "    # Show first 50 chars of each span\n",
    "    preview = span.text[:50] + \"...\" if len(span.text) > 50 else span.text\n",
    "    print(f\"  {i}: [{span.start_char:3d}:{span.end_char:3d}] {preview!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "plugin.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f169feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
